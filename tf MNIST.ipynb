{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(images, targets), (images_valid, targets_valid) = mnist.load_data()\n",
    "images, images_valid= images / 255.0, images_valid / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=images.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On selectionne un echantillon de 12 images au hasard\n",
    "images_demo = images.reshape((-1, 28, 28))\n",
    "select = np.random.randint(images.shape[0], size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# On affiche les images \n",
    "for index, value in enumerate(select):\n",
    "    plt.subplot(3,4,index+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(images_demo[value],cmap=plt.cm.gray_r,interpolation=\"nearest\")\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moyenne et écart-type des images 0.1306604762738429 0.3081078038564622\n",
      "Moyenne et écart-type des images remise à l'échelle 5.1098400794094047e-14 0.956316274213992\n"
     ]
    }
   ],
   "source": [
    "print(\"Moyenne et écart-type des images\", images.mean(), images.std())\n",
    "scaler = StandardScaler()\n",
    "scaled_images = scaler.fit_transform(images.reshape(-1, 28*28))\n",
    "scaled_images_valid = scaler.transform(images_valid.reshape(-1, 28*28))\n",
    "print(\"Moyenne et écart-type des images remise à l'échelle\", scaled_images.mean(), scaled_images.std())\n",
    "\n",
    "scaled_images = scaled_images.reshape(-1, 28, 28, 1)\n",
    "scaled_images_valid = scaled_images_valid.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on crée un dataset pour les manipulations dans tensorflow\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(scaled_images)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices(scaled_images_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "#taille de chaque composante du dataset image de 28 par 28 avec une catégorie, la cible\n",
    "for item in train_dataset:\n",
    "    print(item.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Itération sur le dataset  avec un nombre d'époch et une taille de batch\n",
    "\n",
    "epoch = 1\n",
    "batch_size = 32\n",
    "for batch_training in train_dataset.repeat(epoch).batch(32):\n",
    "    print(batch_training.shape)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on crée maintenant le train set pour tensorflow\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((scaled_images, targets))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((scaled_images_valid, targets_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DatasetV2.batch of <TensorSliceDataset shapes: ((28, 28, 1), ()), types: (tf.float64, tf.uint8)>>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 28, 28, 1) (32,)\n"
     ]
    }
   ],
   "source": [
    "# et on recommence sur le train set des itérations avec epoch et batch\n",
    "epoch = 1\n",
    "batch_size = 32\n",
    "for images_batch, targets_batch in train_dataset.repeat(epoch).batch(batch_size):\n",
    "    print(images_batch.shape, targets_batch.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ConvModel, self).__init__()\n",
    "        # Convolutions\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, 4, activation='relu', name=\"conv1\")\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 3, activation='relu', name=\"conv2\")\n",
    "        self.conv3 = tf.keras.layers.Conv2D(128, 3, activation='relu', name=\"conv3\")\n",
    "        # Flatten the convolution\n",
    "        self.flatten = tf.keras.layers.Flatten(name=\"flatten\")       \n",
    "        # Dense layers\n",
    "        self.d1 = tf.keras.layers.Dense(128, activation='relu', name=\"d1\")\n",
    "        self.out = tf.keras.layers.Dense(10, activation='softmax', name=\"output\")\n",
    "\n",
    "    def call(self, image):\n",
    "        conv1 = self.conv1(image)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv3 = self.conv3(conv2)\n",
    "        flatten = self.flatten(conv3)\n",
    "        d1 = self.d1(flatten)\n",
    "        output = self.out(d1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10384555, 0.0987796 , 0.10573256, 0.10246237, 0.0886752 ,\n",
       "        0.0971439 , 0.10572062, 0.09265122, 0.11254657, 0.09244239]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvModel()\n",
    "model.predict(scaled_images[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on chosit notre métrique de fonction de cout ici par Cross Entropie\n",
    "#et on choisit notre optimizer pour la descente de gradient afin de calculer les poids \n",
    "# optimum dans le réseau entièrement connecté\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on chosit nos métriques pour voir la progression de la convergence du train set et du validation set\n",
    "# Loss : erreur moyenne commise\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "valid_loss = tf.keras.metrics.Mean(name='valid_loss')\n",
    "# Accuracy : précision \n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "valid_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='valid_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on entraine le modèle\n",
    "@tf.function\n",
    "def train_step(image, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Make a prediction on all the batch\n",
    "        predictions = model(image)\n",
    "        # Get the error/loss on these predictions\n",
    "        loss = loss_object(targets, predictions)\n",
    "    # Compute the gradient which respect to the loss\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # Change the weights of the model\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    # The metrics are accumulate over time. You don't need to average it yourself.\n",
    "    train_loss(loss)\n",
    "    train_accuracy(targets, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on passe par la validation\n",
    "@tf.function\n",
    "def valid_step(image, targets):\n",
    "    predictions = model(image)\n",
    "    t_loss = loss_object(targets, predictions)\n",
    "    # Set the metrics for the test\n",
    "    valid_loss(t_loss)\n",
    "    valid_accuracy(targets, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer conv_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      " Batch 59968/60000, Loss: 0.11103404313325882, Accuracy: 96.79000091552734\n",
      "Epoch 1, Valid Loss: 0.052142418920993805, Valid Accuracy: 98.25999450683594\n",
      " Batch 119968/60000, Loss: 0.03674660623073578, Accuracy: 98.861663818359382\n",
      "Epoch 2, Valid Loss: 0.07608325034379959, Valid Accuracy: 97.68000030517578\n",
      " Batch 179968/60000, Loss: 0.022759519517421722, Accuracy: 99.27166748046875\n",
      "Epoch 3, Valid Loss: 0.061213601380586624, Valid Accuracy: 98.43999481201172\n"
     ]
    }
   ],
   "source": [
    "epoch = 3\n",
    "batch_size = 32\n",
    "b = 0\n",
    "for epoch in range(epoch):\n",
    "    # Training set\n",
    "    for images_batch, targets_batch in train_dataset.batch(batch_size):\n",
    "        train_step(images_batch, targets_batch)\n",
    "        template = '\\r Batch {}/{}, Loss: {}, Accuracy: {}'\n",
    "        print(template.format(\n",
    "            b, len(targets), train_loss.result(), \n",
    "            train_accuracy.result()*100\n",
    "        ), end=\"\")\n",
    "        b += batch_size\n",
    "    # Validation set\n",
    "    for images_batch, targets_batch in valid_dataset.batch(batch_size):\n",
    "        valid_step(images_batch, targets_batch)\n",
    "\n",
    "    template = '\\nEpoch {}, Valid Loss: {}, Valid Accuracy: {}'\n",
    "    print(template.format(\n",
    "        epoch+1,\n",
    "        valid_loss.result(), \n",
    "        valid_accuracy.result()*100)\n",
    "    )\n",
    "    valid_loss.reset_states()\n",
    "    valid_accuracy.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    train_loss.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
